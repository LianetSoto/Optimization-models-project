# Optimization Models Project

##  Descripci贸n

Este proyecto implementa y compara dos m茅todos de optimizaci贸n no lineal (Gradiente Descendente y Quasi-Newton BFGS) para minimizar una funci贸n objetivo compleja. El objetivo es analizar el desempe帽o de estos algoritmos en diferentes escenarios, especialmente considerando que la funci贸n tiene un m铆nimo global 煤nico en (0,0) pero no es convexa en todo su dominio.

## Funci贸n Objetivo

```latex
f(x,y) = (e^x + e^y) \cdot \arctan(x^2 + y^2)

##  Instalaci贸n y Configuraci贸n

### Prerrequisitos
- **Python 3.8+**
- **Bibliotecas de Python:**
  - `numpy`
  - `scipy`
  - `matplotlib`
  - `jupyter`

## И Generaci贸n y Ejecuci贸n de Casos de Prueba
Para probar la implementacion de los algoritmos se cuenta con varias opciones:

### Opci贸n 1: Casos Predefinidos
El proyecto incluye 8 casos de prueba organizados en 2 categor铆as:

 **Puntos Cercanos al Origen (0,0)**  
 **Puntos Lejanos al Origen (0,0)**

### Opci贸n 2: Crear Nuevos Casos de Prueba
Puedes modificar el archivo `casos_prueba.json` para agregar nuevos casos.

### Opci贸n 3: Crear Tu Propio Archivo JSON
Crea tu propio archivo JSON personalizado con tus propios casos.

##  Par谩metros Configurables en JSON

Cada caso de prueba acepta estos par谩metros:

```json
{
  "casos_prueba": [
    {
      "name": "Mi-Caso-Personalizado",
      "x0": [x_inicial, y_inicial],
      "learning_rate": 0.01,
      "tolerance": 1e-6,
      "max_iterations": 100,
      "category": "personalizado"
    }
  ]
}
Par谩metros configurables:
x0: Punto inicial [x, y]
learning_rate: Tasa de aprendizaje para Gradiente Descendente
tolerance: Tolerancia para criterio de parada
max_iterations: N煤mero m谩ximo de iteraciones
category: Categor铆a para organizaci贸n


```python
# Ejecutar con tu archivo personalizado
resultados = ejecutar_experimentos("mis_casos_personalizados.json")

# O guardar con un nombre espec铆fico
resultados = ejecutar_experimentos(
    "mis_casos_personalizados.json", 
    "resultados_personalizados.json"
)