# Optimization Models Project

## 游늶 Descripci칩n

Este proyecto implementa y compara dos m칠todos de optimizaci칩n no lineal (Gradiente Descendente y Quasi-Newton BFGS) para minimizar una funci칩n objetivo compleja. El objetivo es analizar el desempe침o de estos algoritmos en diferentes escenarios, especialmente considerando que la funci칩n tiene un m칤nimo global 칰nico en (0,0) pero no es convexa en todo su dominio.

## Funci칩n Objetivo

`f(x,y) = (e틲 + e퉞) arctan(x + y)`

## 游 Instalaci칩n y Configuraci칩n

### Prerrequisitos
- **Python 3.8+**
- **Bibliotecas de Python:**
  - `numpy`
  - `scipy`
  - `matplotlib`
  - `jupyter`

## 游빍 Generaci칩n y Ejecuci칩n de Casos de Prueba
Para probar la implementacion de los algoritmos se cuenta con varias opciones:

### Opci칩n 1: Casos Predefinidos
El proyecto incluye 32 casos de prueba organizados en 2 categor칤as:

游댳 **Puntos Cercanos al Origen (0,0)**  
游댲 **Puntos Lejanos al Origen (0,0)**

### Opci칩n 2: Crear Nuevos Casos de Prueba
Puedes modificar el archivo `casos_prueba.json` para agregar nuevos casos.

### Opci칩n 3: Crear Tu Propio Archivo JSON
Crea tu propio archivo JSON personalizado con tus propios casos.

## 游늵 Par치metros Configurables en JSON

Cada caso de prueba acepta estos par치metros:

```json
{
  "casos_prueba": [
    {
      "name": "Mi-Caso-Personalizado",
      "x0": [x, y_inicial],
      "learning_rate": 0.01,
      "tolerance": 1e-6,
      "max_iterations": 100,
      "category": "personalizado"
    }
  ]
}
Par치metros configurables:
x0: Punto inicial [x, y]
learning_rate: Tasa de aprendizaje para Gradiente Descendente
tolerance: Tolerancia para criterio de parada
max_iterations: N칰mero m치ximo de iteraciones
category: Categor칤a para organizaci칩n
```

```python
# Ejecutar con tu archivo personalizado
resultados = ejecutar_experimentos("mis_casos_personalizados.json")

# O guardar con un nombre espec칤fico
resultados = ejecutar_experimentos(
    "mis_casos_personalizados.json", 
    "resultados_personalizados.json"
)